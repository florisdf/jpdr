{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be43e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1388ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jpdr.utils.crop_batch import crop_and_batch_boxes\n",
    "from jpdr.utils.boxes import (\n",
    "    make_boxes_batchable, shift_boxes_to_fit_img\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddddfd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jpdr.datasets import GroZi3k\n",
    "import jpdr.transforms as T\n",
    "from data import get_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a696be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 800\n",
    "\n",
    "dl_train_det, dl_train_recog, dl_val = get_dataloaders(\n",
    "    'tonioni', img_size, normalize=False,\n",
    "    shuffle_train=False, num_workers=8,\n",
    "    train_crop_how=\"center\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b028b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "from random import randint\n",
    "\n",
    "def rand_chan():\n",
    "    return randint(0, 255)\n",
    "\n",
    "def rand_rgb():\n",
    "    return (rand_chan(), rand_chan(), rand_chan())\n",
    "\n",
    "def draw_boxes(im, boxes, labels=None):\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    if labels is None:\n",
    "        labels = [\"\"]*len(boxes)\n",
    "\n",
    "    for box, label in zip(boxes, labels):\n",
    "        draw.rectangle(tuple(box), width=3, outline=rand_rgb())\n",
    "        draw.text((box[0], box[1]), label, fill=\"white\")\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "raw",
   "id": "045b2767",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "all_boxes = torch.vstack([\n",
    "    t['boxes']\n",
    "    for _, targets in dl_train_recog\n",
    "    for t in targets\n",
    "])\n",
    "\n",
    "widths = all_boxes[:, ::2].diff().squeeze()\n",
    "heights = all_boxes[:, 1::2].diff().squeeze()\n",
    "\n",
    "widths.mean(), heights.mean()\n",
    "# (tensor(195.8140), tensor(229.2918))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6e9156",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, BoundedIntText\n",
    "from torchvision.transforms import ToPILImage\n",
    "from jpdr.utils.image_tools import show_ims\n",
    "\n",
    "from training_steps_crop_batch import create_crop_batch\n",
    "\n",
    "\n",
    "def build_get_crop_boxes(dl, callback):\n",
    "    def get_crop_boxes(\n",
    "        batch=0, crop_size='mean', iou_thresh=0.95,\n",
    "        crop_box_max_rand_shift=0,\n",
    "        crop_box_max_out_pct=0.0,\n",
    "        min_tgt_area_pct=0.5,\n",
    "    ):\n",
    "        for i, (x, targets) in enumerate(dl):\n",
    "            if i == batch:\n",
    "                break\n",
    "\n",
    "        if isinstance(crop_size, int):\n",
    "            crop_size = (crop_size, crop_size)\n",
    "\n",
    "        crops, crop_tgts, crop_boxes = create_crop_batch(\n",
    "            x, targets,\n",
    "            crop_size,\n",
    "            crop_box_max_rand_shift,\n",
    "            iou_thresh,\n",
    "            crop_box_max_out_pct,\n",
    "            min_tgt_area_pct,\n",
    "            return_crop_boxes=True\n",
    "        )\n",
    "        callback(x, targets, crops, crop_tgts, crop_boxes)\n",
    "\n",
    "    return get_crop_boxes\n",
    "\n",
    "\n",
    "def interact_crop_boxes(dl, callback):\n",
    "    interact(\n",
    "        build_get_crop_boxes(dl, callback),\n",
    "        batch=BoundedIntText(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=len(dl),\n",
    "        ),\n",
    "        crop_size=['mean', 'max', 300, 600, 800],\n",
    "        iou_thresh=(0.0, 1.0),\n",
    "        crop_box_max_out_pct=(0.0, 1.0),\n",
    "        min_tgt_area_pct=(0.0, 1.0),\n",
    "    )\n",
    "\n",
    "\n",
    "def show_crop_boxes(x, targets, crops, crop_tgts, crop_boxes):\n",
    "    to_pil = ToPILImage()\n",
    "\n",
    "    ncols = 2\n",
    "    pil_ims = [\n",
    "        draw_boxes(to_pil(t), boxes)\n",
    "        for t, boxes in zip(x, crop_boxes)\n",
    "    ]\n",
    "    show_ims(pil_ims, figsize=(40, 40*len(pil_ims)/ncols), columns=ncols)\n",
    "\n",
    "\n",
    "def show_crops_with_targets(x, targets, crops, crop_tgts, crop_boxes):\n",
    "    tgt_boxes = [\n",
    "        tgt['boxes']\n",
    "        for tgt in crop_tgts\n",
    "    ]\n",
    "    tgt_ids = [\n",
    "        tgt['product_ids']\n",
    "        for tgt in crop_tgts\n",
    "    ]\n",
    "\n",
    "    to_pil = ToPILImage()\n",
    "    pil_crops = [to_pil(crop) for crop in crops]\n",
    "\n",
    "    crop_h, crop_w = crops.shape[-2:]\n",
    "    ncols = 5\n",
    "    scale = 0.1\n",
    "\n",
    "    show_ims([\n",
    "        draw_boxes(to_pil(t), boxes, [str(int(label)) for label in labels])\n",
    "        for t, boxes, labels in zip(crops, tgt_boxes, tgt_ids)\n",
    "    ], figsize=(crop_w*scale, (len(pil_crops)*crop_h/ncols)*scale/4), columns=ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f47e137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2a0532ab4d4dcf8bb9633370c728c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(BoundedIntText(value=0, description='batch', max=49), Dropdown(description='crop_size', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact_crop_boxes(dl_train_recog, show_crop_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b232e56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eead4a1e2ec44790a6f95e249bce7b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(BoundedIntText(value=0, description='batch', max=49), Dropdown(description='crop_size', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact_crop_boxes(dl_train_recog, show_crops_with_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
